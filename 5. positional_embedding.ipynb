{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b19d473c",
   "metadata": {},
   "source": [
    "### **Why do we even need positional embeddings?**\n",
    "\n",
    "Transformers process input all at once, not word-by-word like RNNs.\n",
    "\n",
    "That means by default the model sees this as the same:\n",
    "\n",
    "❌ “dog bites man”\n",
    "\n",
    "❌ “man bites dog”\n",
    "\n",
    "Because self-attention has no sense of order.\n",
    "\n",
    "So we must tell the model:\n",
    "\n",
    "**which word came first, second, third, etc.**\n",
    "\n",
    "That information is added using positional embeddings.\n",
    "\n",
    "### **Positional embeddings are vectors added to token embeddings to tell the model the position (order) of each token in the sentence.**\n",
    "\n",
    "So final input becomes:\n",
    "\n",
    "Input Vector = Token Embedding + Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f999120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
