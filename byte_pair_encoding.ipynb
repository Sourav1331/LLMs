{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c83ebaaf",
   "metadata": {},
   "source": [
    "### There are three types of tokenization algorithms:\n",
    "\n",
    "**1. Word-based tokenizer** - Each word in a sentence is treated as one token\n",
    "\n",
    "*Example* : text = \"I love machine learning\"\n",
    "\n",
    "        tokenized_text = [\"I\", \"love\", \"machine\", \"learning\"]\n",
    "\n",
    "**2. Character-based tokenizer** - Individual characters are treated as tokens\n",
    "\n",
    "*Example* : text = \"AI\"\n",
    "\n",
    "        tokenized_text = [\"A\", \"I\"]\n",
    "\n",
    "**3. Subword-based tokenizer** - Words are broken down into smaller meaningful subword units (Byte Pair Encoding/BPE is an example of this type)\n",
    "\n",
    "*Example* : text = \"unhappiness\"\n",
    "\n",
    "        tokenized_text = [\"un\", \"happi\", \"ness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1d726f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiktoken Version :  0.11.0\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import importlib\n",
    "print(\"Tiktoken Version : \", importlib.metadata.version('tiktoken'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d76eba",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding\n",
    "\n",
    "### Byte Pair Encoding is a subword tokenization technique that repeatedly merges the most frequent adjacent symbol pairs to form a compact and efficient vocabulary.\n",
    "\n",
    "Example:    \n",
    "\n",
    "        Corpus:                  low lower lowest\n",
    "\n",
    "        Initial tokens:            l o w   l o w e r   l o w e s t\n",
    "\n",
    "        After merges:              low   low er   low est\n",
    "\n",
    "        Final tokens might be:     [\"low\", \"er\", \"est\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b29eb3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2f83997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  Hi, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n",
      "Token IDs:  [17250, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hi, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "token_ids = tokenizer.encode(text, allowed_special={'<|endoftext|>'})  # text -> tokens -> token IDs\n",
    "print(\"Text: \", text)\n",
    "print(\"Token IDs: \", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39fbe36c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = tokenizer.decode(token_ids)  # token IDs -> tokens -> text\n",
    "string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2efc48",
   "metadata": {},
   "source": [
    "**<|endoftext|>** token is used to mark sequence boundaries, prevent mixing of documents during training, and provide a natural stopping signal during text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc2bb80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
