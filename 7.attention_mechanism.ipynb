{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e37c65cf",
   "metadata": {},
   "source": [
    "### **Attention is a technique that lets a model focus on the most relevant parts of the input when producing an output — instead of treating everything as equally important.**\n",
    "\n",
    "- Think of it like how humans read:\n",
    "When you read a sentence, you don’t remember every word equally — your brain pays more attention to the important words.\n",
    "\n",
    " Simple Example (Human-style):\n",
    "\n",
    "Sentence: “The cat sat on the mat because it was tired.”\n",
    "\n",
    "Question: What does “it” refer to?\n",
    "\n",
    "Your brain instantly links “it” → “cat”, not “mat”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efae3ce1",
   "metadata": {},
   "source": [
    "### **Self-attention means:**\n",
    "\n",
    "- Words attend to other words in the same sentence\n",
    "\n",
    "- Every word understands its context\n",
    "\n",
    "*Example:*\n",
    "\n",
    "“I went to the bank to deposit money”\n",
    "\n",
    "The word “bank” attends strongly to “deposit” and “money”, not “river”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9c3a12",
   "metadata": {},
   "source": [
    "### **Types of attention mechanisms:-** \n",
    "1. Simplified Self-Attention (Each word decides which other words are important)\n",
    "\n",
    "2. Self-Attention (Each word uses query, key, and value to understand context from all words)\n",
    "\n",
    "3. Causal Attention (Each word can attend only to past words, not future ones)\n",
    "\n",
    "4. Multi-Head Attention (Multiple attention heads look at the same sentence from different perspectives)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b249d31",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
